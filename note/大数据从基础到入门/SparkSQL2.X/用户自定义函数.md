# 目录 #

- [基本自定义函数](#1)
- [自定义聚合函数](#2)

***

<h4 id='1'>基本自定义函数</h4>

```
spark.sqlContext.udf.register("concatStr",(s1:String, s2:String) => s1 + s2)

spark.sql("select concatStr(ename, job) from emp").show
```

***

<h4 id='1'>自定义聚合函数</h4>

```
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.expressions.MutableAggregationBuffer;
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.List;

public class UdafDemo extends UserDefinedAggregateFunction {

    @Override
    public StructType inputSchema() {
        // 输入数据的类型
        List<StructField> structFields = new ArrayList<>();
        structFields.add(DataTypes.createStructField("line", DataTypes.StringType, false));
        return DataTypes.createStructType(structFields);
    }

    @Override
    public StructType bufferSchema() {
        // 缓存字段的数据类型
        List<StructField> structFields = new ArrayList<>();
        structFields.add(DataTypes.createStructField("fields1", DataTypes.IntegerType, false)); // 用于记录总数
        structFields.add(DataTypes.createStructField("fields2", DataTypes.IntegerType, false)); // 用于记录总和
        return DataTypes.createStructType(structFields);
    }

    @Override
    public DataType dataType() {
        // 计算后的结果
        return DataTypes.DoubleType;
    }

    @Override
    public boolean deterministic() {
        // 判断输入输出类型是否一致
        return false;
    }

    @Override
    public void initialize(MutableAggregationBuffer buffer) {
        // 对辅助字段进行初始化
        buffer.update(0, 0);
        buffer.update(1, 0);
    }

    @Override
    public void update(MutableAggregationBuffer buffer, Row input) {
        // 修改辅助字段的值
        buffer.update(0, buffer.getInt(0) + 1); // 计数
        buffer.update(1, buffer.getInt(1) + Integer.parseInt(input.getString(0))); // 对input字段求和

    }

    @Override
    public void merge(MutableAggregationBuffer buffer1, Row buffer2) {
        // 对分区结果进行合并
        buffer1.update(0, buffer1.getInt(0) + buffer2.getInt(0));
        buffer1.update(1, buffer1.getInt(1) + buffer2.getInt(1));
    }

    @Override
    public Object evaluate(Row buffer) {
        // 最终计算的结果
        return buffer.getInt(1) / (double) buffer.getInt(0); // 总和/总数量
    }

    public static void main(String[] args) {
        SparkConf conf = new SparkConf();
        conf.setMaster("local").setAppName("UdafDemo");

        JavaSparkContext sc = new JavaSparkContext(conf);

        SQLContext sqlContext = new SQLContext(sc);

        sqlContext.udf().register("my_avg", new UdafDemo());

        JavaRDD<String> lines = sc.textFile("students");
        JavaRDD<Row> rows = lines.map(line -> RowFactory.create(line.split(" ")));

        List<StructField> structFields = new ArrayList<>();
        structFields.add(DataTypes.createStructField("id", DataTypes.StringType, false));
        structFields.add(DataTypes.createStructField("name", DataTypes.StringType, true));
        structFields.add(DataTypes.createStructField("age", DataTypes.StringType, true));
        StructType structType = DataTypes.createStructType(structFields);

        Dataset ds = sqlContext.createDataFrame(rows, structType);
        ds.registerTempTable("students");

        sqlContext.sql("select my_avg(age) from students group by name").show();

        sc.stop();
    }
}
```
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

# 目录 #

- ## [神经网络概述](#L1)
- ## [神经网络发展史](#L2)
- ## [从0到1——单层感知器](#L3)

********

<h2 id='L1'>神经网络概述</h2>

神经网络
- 大脑，神经元
- 应用：图像识别、自动驾驶、人工智能

神经网络种类
- 神经网络基础
    - 单层感知器，线性神经网络，BP神经网络，Hopfield神经网络等
- 神经网络进阶
    - 玻尔兹曼机，受限玻尔兹曼机，递归神经网络等
- 深度学习网络
    - 深度置信网络，卷积神经网络，深度残差网络，LSTM网络等
- 深度网络应用
    - 传统的数据挖掘与机器学习问题，手写体识别，图像识别，自然语言处理，人工智能等
- 编程语言
    - python，R，matlab，java等
- 深度学习实现框架
    - Tensorflow，Caffe，Torch等
- 发展过程
    - 单层感知器
        - BP神经网络
            - 递归神经网络(RNN)
                -LSTM
            - 卷积神经网络
                - 残差神经网络
        - Hopfield神经网络
            - 玻尔兹曼机
                - 受限玻尔兹曼机
    - BP神经网络 + 受限玻尔兹曼机 ==> 深度置信网络

********

<h2 id='L2'>神经网络发展史</h2>

- 启蒙时期（1890-1969）
    - 1890年，心理学家William James，《心理学原理》，神经细胞受到刺激激活后把刺激传播到另一个神经细胞，神经细胞激活是所有输入叠加的结果
    - 1943年，神经元的数学描述和结构，M-P模型，人工神经网络（ANN）的起点
    - 1949年，《行为组织学》，神经元权值
    - 1958年，三层网络特性的神经网络结构，“感知器”
    - 1969年，《感知器》，简单神经网络智能运用于线性问题的求解，需要隐层，多层网络
- 低潮时期（1969-1982）
- 复兴时期（1982-1986）
    - 1982年，Hopfield提出Hopfield神经网络，引用了物理学的分析方法
    - 1985年，借助统计物理学的概念和方法，提出了一种随机网络模型——玻尔兹曼机。一年后改进为受限玻尔兹曼机
    - 1986年，BP算法（多层感知器的误差反向传播算法）
- 新时期（1986-）
    - 1987年6月，首届国际神经网络学术会议

********

<h2 id='L3'>单层感知器</h2>

单层感知器
- 输入节点：x1,x2,x3
- 输出节点：y
- 权向量：w1,w2,w3
- 偏置因子：b
- 激活函数：sign(X)
    - $X = \sum_i{w_i x_i + b}$
    - $x_i,w_1... \rightarrow f(X) \rightarrow y$
- 把偏置当做特殊权值
    - $x_1,w_1...,x_0=1,b \rightarrow f(\sum_i{w_i x_i}) \rightarrow y$

感知器学习规则
- 1958年，美国学者Frank Rosenblatt首次定义了一个具有单层计算单元的神经网络结构，称为Perceptron（感知器）
- 感知器的学习规则规定，学习信号等于神经元期望输出（教室信号）与实际输出之差
    - $r = d_j - o_j$
        - $d_j$为期望输出
        - $o_j = f({W_j}^T X)$
            - W权值矩阵，w权值，T转置，${W_j}^T X = \sum_i{w_i x_i}$
- 感知器采用了符号函数作为转移函数
    - $
    f({W_j}^T X) = sign({W_j}^T X) 
    = \begin{cases}
        1, {W_j}^T X \geq 0 \\
        -1, {W_j}^T X < 0 \\
    \end{cases}
    $
    - 权值调整公式为：
        - $\Delta W_j = \eta \left[d_j - sgn({W_j}^T X)\right] X$
        - $\Delta w_{ij} = \eta \left[d_j - sgn({W_j}^T X)\right] x_i, i=0,1,...,n$        
        - $\eta$： 学习信号，学习率
            - $0 < \eta < 1$
            - 学习率太大，容易造成权值不稳定
            - 学习率太小，权值调整太慢，迭代次数太多
        - 当实际输出与期望值相同时，权值不需要调整
        - 在有误差存在的情况下，由于$d_j$和$sgn({W_j}^T X)$ $\in \{-1,1\}$,权值调整公式可简化为：$\Delta W_j = \pm 2\eta X$
- 感知器学习规则只适用于二进制神经元，初始权值可以取任意值
- 是一种有导师学习
- 是研究其他神经网络的基础

收敛条件
- 误差小于某个预先设定的较小的值
- 两次迭代之间的权值变化已经很小
- 设定最大迭代次数，当迭代超过最大次数就停止
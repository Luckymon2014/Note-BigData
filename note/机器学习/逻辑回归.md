## 主要内容
[1. 逻辑回归模型](#1)
[2. 逻辑回归的损失函数](#2)
[3. 逻辑回归的模型求解](#3)
[4. 逻辑回归算法实战](#4)

---

<h4 id='1'>逻辑回归模型</h4>

- 逻辑回归也被称为对数几率回归
- 该算法是分类算法
- 用了和回归类似的方法来解决分类问题
- 用条件概率分布的形式表示P(Y|X)
    - 随机变量X取值为n维实数向量
    - Y取值为0或1

逻辑回归模型
- 定义
    $$P(Y=1|x) = \frac{exp(\omega·x+b)}{1+exp(\omega·x+b)}$$
    $$P(Y=0|x) = \frac{1}{1+exp(\omega·x+b)}$$
- 二分类问题
    - 输出为y={0,1}
    - 线性回归模型产生的预测值$z=\omega x+b$是实数
    - 希望有一个理想的阶跃函数来实现z值到0/1值的转化：
    $$\phi(z)=\begin{cases}
        0, \text{ if }z<0\\
        0.5, \text{ if }z=0\\
        1, \text{ if }z>0\\
    \end{cases}$$
    - 但是该函数并不连续，希望有一个单调可微的函数来使用，于是找到了Sigmoid函数代替：
    $$\phi(z)=\frac{1}{1+e^{-z}}$$
    - 有了Sigmoid函数之后，由于其取值范围为[0,1]，可以将其视为类1的后验概率估计p(y=1|x)
    - 即，如果有了一个测试点x，可以用Sigmod函数算出来的结果当做该点x属于类别1的概率大小
    - 把Sigmode函数计算得到的值大于等于0.5的归类为类别1，小于0.5的归类为类别0

---

<h4 id='2'>逻辑回归的损失函数</h4>

- 定义
    $$J(\omega) = \sum_i{\frac{1}{2}(\phi(z^{(i)})}-y^{(i)})^2$$
- $z^{(i)} = \omega^Tx^{(i)} + b_i$表示第i个样本点
- $y^{(i)}$表示第i个样本的真实值
- $\phi(z^{(i)})$表示第i个样本的预测值
- 将$\phi(z^{(i)}) = \frac{1}{1+e^{-z{(i)}}}$代入，得到一个非凸函数，意味着代价函数有许多的局部最小值，这不利于求解
- 换个思路
- 由于$\phi(z)$可以视为类1的后验估计，因此有：
    $$p(y=1|x;\omega)=\phi(\omega^Tx+b)=\phi(z)$$
    $$p(y=0|x;\omega)=1-\phi(z)$$
- 其中，$p(y=1|x;\omega)$表示给定$\omega$，x点y=1的概率大小。于是上面两式写成一般形式：
    $$p(y|x;\omega)=\phi(z)^y(1-\phi(z))^{(1-y)}$$
- 以上过程说明，最大似然估计与误差平方和等价
- 因此逻辑回归的损失函数可以用最大似然函数进行估计

参数估计
- 用极大似然估计来估计出参数$\omega$：
    $$L(\omega)=\prod_{i=1}^n{p(y^{(i)}|x^{(i)};\omega)}=\prod_{i=1}^n{\phi(z^{(i)})^{y^{(i)}}(1-\phi(z^{(i)}))^{(1-y^{(i)})}}$$
- 简化运算，两边去对数:
    $$l(\omega)=\ln L(\omega) = \sum_{i=1}^n{y^{(i)} \ln{(\phi(z^{(i)}))}} + (1-y^{(i)})\ln{(1-\phi(z^{(i)}))}$$
- 目标：使得$l(\omega)$最大的$\omega$，因此代价函数即：$J(\omega) = -l(\omega)$

---

<h4 id='3'>逻辑回归的模型求解</h4>

Sigmod函数一个很好的性质
    $$\phi'(z)=\phi(z)(1-\phi(z))$$

利用梯度下降法求解
    $$\omega = \omega + \Delta \omega, \Delta \omega = -\eta\nabla{J(\omega)}$$
    $$\omega_j = \omega_j + \Delta \omega_j, \Delta \omega_j = -\eta\frac{\partial{J(\omega)}}{\partial\omega_j}$$
- $\omega_j$：第j个特征的权重
- $\eta$：学习率，控制步长
- 代入损失函数：
    $$\frac{\partial{J(\omega)}}{\partial\omega_j} 
    = -\sum_{i=1}^n{\left(
        y^{(i)} \frac{1}{\phi(z^{(i)})}
        - (1-y^{(i)})\frac{1}{1-\phi(z^{(i)})}
        \right)}
        \frac{\partial{\phi{z^{(i)}}}}{\partial\omega_j}$$
    $$= -\sum_{i=1}^n{\left(
        y^{(i)} \frac{1}{\phi(z^{(i)})}
        - (1-y^{(i)})\frac{1}{1-\phi(z^{(i)})}
        \right)}
        \phi(z^{(i)})(1-\phi(z^{(i)}))\frac{\partial{z^{(i)}}}{\partial\omega_j}$$
    $$= -\sum_{i=1}^n{\left(
        y^{(i)}(1-\phi(z^{(i)}))
        - (1-y^{(i)})\phi(z^{(i)})
        \right)}
        x_j^{(i)}$$
    $$= -\sum_{i=1}^n{\left(y^{(i)} - \phi(z^{(i)})\right)}x_j^{(i)}$$


---

<h4 id='4'>逻辑回归算法实战</h4>
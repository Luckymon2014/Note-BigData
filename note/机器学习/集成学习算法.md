## 主要内容
[1. 集成学习方法原理](#1)
[2. AdaBoost算法原理](#2)
[3. RandomForest算法原理](#3)
[4. GBDT算法原理](#4)

---

<h4 id='1'>集成学习方法原理</h4>

集成学习两种策略
- Boosting
    - 通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能
        - 提高训练错的样本的权重
        - 减少训练对的样本的权重
        - 后一次训练基于前一次训练的结果
- Bagging
    - 通过从原始训练集中抽取n个样本的训练集，共进行k次，得到k个样本集，每个样本集单独训练，最后通过投票的方式得到分类结果，结果是回归方法则采用平均值作为预测结果
        - 训练结果多数服从少数
        - 训练不分先后，可以并行训练，互相独立

对比
- 样本选择
    - Boosting：每一轮的训练集不变，只是每个样本的权重发生变化
    - Bagging：训练集在原始集中有放回选取的，各轮训练选出的训练集之间互相独立
- 样例权重
    - Boosting：根据错误率不断调整
    - Bagging：均匀取样，每个样例权重相等
- 预测函数
    - Boosting：误差小的，权重更大
    - Bagging：权重相等
- 并行计算
    - Boosting：各个预测函数只能顺序生成
    - Bagging：各个预测函数可以并行生成

将不同的分类算法套入到集成学习算法框架中，一定程度上提高了原单一分类器的分类效果，但是也增大了计算量
- Bagging + 决策树 = 随机森林（RandomForest）
- AdaBoost + 决策树 = 提升树
- Gradient Boosting + 决策树= GBDT
    - XGBoost是GBDT的一种实现方式

---

<h4 id='2'>AdaBoost算法</h4>

提升方法的两个问题
1. 每一轮如何改变训练数据的权重或概率分布
2. 如何将各个弱分类器组合成一个强分类器

AdaBoost
- 提高前一轮弱分类器分类错误的样本的权重，降低正确分类的样本权重
- 弱分类器的组合，采用加权多数表决：加大分类误差率小的弱分类器的权重，使其在表决中起较大作用；减小分类误差率大的弱分类器的权重，使其在表决中起较小的作用

AdaBoost算法
- 输入：
    - 训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in X \subseteq R^n$，类标$y_i \in Y = \{-1,+1\}$
    - 弱分类器迭代次数M
- 输出：
    - 强分类器$G(x)$
- 执行步骤：
    1. 初始化训练数据的权值分布：
    $$D_1=(\omega_{11},...,\omega_{1i},...,\omega_{1N}),\ \omega_{1i}=\frac{1}{N},\ i=1,2,...,N$$
    2. 对$m=1,2,...,M$
    - 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器：
        $$G_m(x):X \rightarrow \{-1,+1\}$$
        - 可以理解为一个简单的决策树
    - 计算$G_m(x)$在训练数据集上的分类误差率：
        $$e_m=P(G_m(x) \neq y_i)=\frac{\sum_{i=1}^n{I(G_m(x_i) \neq y_i)}}{N}$$
        - $I$：指示函数，条件满足时为1，否则为0
    - 计算$G_m(x)$的系数:
        $$\alpha_m=\frac{1}{2}\log_e\frac{1-e_m}{e_m}$$
        - 误差率${e_m}$越大，系数$\alpha_m$越小
    - 更新训练数据的权值分布：
        $$D_{m+1}=(\omega_{m+1,1},...,\omega_{m+1,i},...,\omega_{m+1,N})$$
        $$Z_m=\sum_{i=1}^N{\omega_{mi}exp(-\alpha_my_iG_m(x_i))}$$
        $$\omega_{m+1,i}=\frac{\omega_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))$$
        - 误差率${e_m}$越大，系数$\alpha_m$越小，$\omega_{m+1,i}$越大
    - 构建基本分类器的线性组合:
        $$f(x)=\sum_{i=1}^M{\alpha_mG_m(x)}$$
    3. 最终分类器为：
        $$G(x)=sign(f(x))=sign\left(\sum_{i=1}^M{\alpha_mG_m(x)}\right)$$

---

<h4 id='3'>RandomForest算法</h4>

- Bagging + 决策树
- 输入：
    - 训练数据集$S=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in X \subseteq R^n$
    - 弱分类器迭代次数T
- 输出：
    - 强分类器$F(x)$
- 执行步骤：
    1. 对于$t=1,2,...,T$
    - 对训练集进行第t次有放回随机采样，共采样N次，得到包含N个样本的采样集$D_t$
    - 用采样集$D_t$训练第t个弱学习器$G_t(x)$
        - 弱学习器$G_t(x)$，即决策树CART
    2. 如果是分类算法预测，则T个弱学习器投出最多票数的类别或类别之一为最终类别。如果是回归算法预测，则T个弱学习器得到的回归结果进行算术平均，得到的值为最终的模型输出。

优点
- 训练可以高度并行化，大样本训练速度有优势
- 可以随机选择决策树节点划分特征，因此在样本特征维度很高的时候，仍然能高效的训练模型
- 训练后可以给出各个特征对于输出的重要性
- 由于采用了随机采样，训练出的模型方差小，泛化能力强
- 相对于Boosting系列的Adaboost和GBDT，RF实现比较简单
- 对部分特征缺失不敏感

缺点
- 在噪音比较大的样本集上，容易陷入过拟合
- 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型效果
- 相对于GBDT算法来说，想要达到一个比较好的效果，弱分类器的个数远高于GBDT，也就是说RF模型在高维大数据集上训练出的模型太大

---

<h4 id='4'>GBDT算法</h4>

Adaboost：利用权重提升树
GBDT：利用残差提升树

提升树思想
- $T_1,T_2,...,T_T$
- 样本$y_1,y_2,...,y_N$，训练结果$\hat{y}$
- 残差$y_i'=\hat{y_i}-y_i$
- 用残差代替样本，不断的用残差拟合样本

提升树算法
- 输入：
    - 训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in X \subseteq R^n$，类标$y_i \in Y \subseteq R^n$
- 输出：
    - 提升树$f_k(x)$
- 执行步骤：
    1. 初始化$f_0(x)=0$
    2. 对$k=1,2,...,K$依次进行循环迭代：
    - 计算每个样本的残差：
        $$r_{ki} = y_i-f_{k-1}(x_i), i=1,2,...,N$$
    - 拟合残差学习一个回归树，得到$T(x,\theta_K)$
    - 更新$f_k(x)=f_{k-1}(x)+T(x,\theta_K)$
    3. 得到回归树：
        $$f_K(x) = \sum_{k=1}^K{T(x,\theta_k)}$$

GBDT算法原理
- 提升树利用加法模型与前向分步算法实现学习的优化过程
    - 当损失函数是平方损失和指数损失时，每一步优化很简单
    - 对于一般损失函数而言，优化并不那么容易
- Freidman提出了梯度提升算法，其关键是利用损失函数的负梯度在当前模型的值：
    $$-\left[\frac{\partial{L(y,f(x_i))}}{\partial{f(x_i)}}\right]_{f(x)=f_{m-1}(x)}$$
    作为残差的近似值，拟合回归树
    - 简化优化树的计算，近似值的误差经过多次迭代后可以忽略不计